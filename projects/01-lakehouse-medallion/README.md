# ðŸ—ï¸ Lakehouse Medallion Architecture â€” Azure Databricks + ADLS Gen2

> Pipeline end-to-end con arquitectura Bronze/Silver/Gold, ingesta multi-fuente, transformaciones con Delta Live Tables, gobierno con Unity Catalog y visualizaciÃ³n en Power BI.

[![Azure](https://img.shields.io/badge/Azure-0078D4?style=flat-square&logo=microsoftazure&logoColor=white)](#)
[![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=flat-square&logo=databricks&logoColor=white)](#)
[![Delta Lake](https://img.shields.io/badge/Delta_Lake-003366?style=flat-square)](#)
[![Terraform](https://img.shields.io/badge/Terraform-7B42BC?style=flat-square&logo=terraform&logoColor=white)](#)
[![CI/CD](https://img.shields.io/badge/Azure_DevOps-0078D7?style=flat-square&logo=azuredevops&logoColor=white)](#)

---

## ðŸ“‹ Tabla de Contenidos

- [Contexto del Proyecto](#-contexto-del-proyecto)
- [Arquitectura](#-arquitectura)
- [Servicios Azure](#-servicios-azure-utilizados)
- [Flujo de Datos](#-flujo-de-datos)
- [Estructura del Repositorio](#-estructura-del-repositorio)
- [Deployment](#-deployment)
- [Costos Estimados](#-costos-estimados)
- [MÃ©tricas de Performance](#-mÃ©tricas-de-performance)
- [Mejoras Futuras](#-mejoras-futuras)

---

## ðŸŽ¯ Contexto del Proyecto

### Problema

Empresa del sector seguros con **10 fuentes de datos heterogÃ©neas** (core Oracle on-premise, CRM Dynamics 365, SQL Server, APIs REST, archivos actuariales) y cero visibilidad unificada. Los reportes regulatorios tomaban **3 semanas manuales** y los datos llegaban al equipo de actuariado con **7 dÃ­as de retraso**.

### Objetivo

Construir un **Data Lakehouse** centralizado sobre Azure que:
- Consolide todas las fuentes en una plataforma Ãºnica gobernada
- Automatice reportes regulatorios (de 3 semanas a < 30 min)
- Proporcione datos frescos (< 1 hora) al equipo de actuariado y BI
- Implemente calidad de datos automatizada en cada capa

### VolÃºmenes

| MÃ©trica | Valor |
|---------|-------|
| Datos histÃ³ricos | 3 TB |
| Registros nuevos/dÃ­a | ~25 millones |
| Fuentes de datos | 10 |
| Usuarios BI | 50+ |
| RetenciÃ³n regulatoria | 10 aÃ±os |

---

## ðŸ›ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FUENTES DE DATOS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Oracle  â”‚ SQL Serverâ”‚ Dynamics â”‚  APIs     â”‚  Event   â”‚  Archivos â”‚
â”‚  (Core)  â”‚(Siniestrosâ”‚  365     â”‚  REST     â”‚  Hubs    â”‚  Excel    â”‚
â”‚          â”‚          )â”‚  (CRM)   â”‚           â”‚(Eventos) â”‚(Actuarial)â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚           â”‚          â”‚           â”‚          â”‚           â”‚
     â–¼           â–¼          â–¼           â–¼          â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INGESTA (Azure Data Factory)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚Self-hostedâ”‚ â”‚   Copy   â”‚ â”‚ Fivetran â”‚ â”‚ Azure    â”‚ â”‚   ADF    â”‚â”‚
â”‚  â”‚    IR     â”‚ â”‚ Activity â”‚ â”‚  Sync    â”‚ â”‚Functions â”‚ â”‚ SSIS IR  â”‚â”‚
â”‚  â”‚ (Oracle)  â”‚ â”‚ (SQL Srv)â”‚ â”‚(Dynamics)â”‚ â”‚ (APIs)   â”‚ â”‚ (Legacy) â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AZURE DATA LAKE STORAGE GEN2 (ADLS)                   â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚    BRONZE     â”‚  â”‚    SILVER     â”‚  â”‚     GOLD      â”‚          â”‚
â”‚  â”‚  (Raw Data)   â”‚â”€â”€â”‚  (Cleaned)    â”‚â”€â”€â”‚  (Business)   â”‚          â”‚
â”‚  â”‚  Delta Lake   â”‚  â”‚  Delta Lake   â”‚  â”‚  Delta Lake   â”‚          â”‚
â”‚  â”‚               â”‚  â”‚  + DLT        â”‚  â”‚  + Z-ordering â”‚          â”‚
â”‚  â”‚  JSON,CSV,    â”‚  â”‚  Expectations â”‚  â”‚  + Aggregates â”‚          â”‚
â”‚  â”‚  Parquet,Avro â”‚  â”‚  Dedup, MDM   â”‚  â”‚  Pre-computed â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                 â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
              â”‚                                   â”‚
              â–¼                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Databricks SQL      â”‚          â”‚        Power BI          â”‚
â”‚  Warehouse (Pro)     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Dashboards & Reportes   â”‚
â”‚  Photon Engine       â”‚          â”‚  DirectQuery + Import    â”‚
â”‚  Result Caching      â”‚          â”‚  Regulatorios SBS        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Unity Catalog     â”‚
â”‚  Permisos por equipo â”‚
â”‚  Linaje automÃ¡tico   â”‚
â”‚  External Locations  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â˜ï¸ Servicios Azure Utilizados

| Servicio | Rol | JustificaciÃ³n |
|----------|-----|---------------|
| **ADLS Gen2** | Almacenamiento central del Data Lake | Namespace jerÃ¡rquico, ACLs granulares, lifecycle policies (Hotâ†’Coolâ†’Archive) |
| **Azure Data Factory** | OrquestaciÃ³n e ingesta | Self-hosted IR para Oracle on-premise; Copy Activity con CDC para SQL Server; parametrizaciÃ³n para reutilizar pipelines |
| **Azure Databricks (Premium)** | Procesamiento y transformaciones | Photon Engine 5x mÃ¡s rÃ¡pido que Spark estÃ¡ndar; DLT con expectations para calidad regulatoria; Unity Catalog para gobierno |
| **Delta Live Tables (DLT)** | ETL declarativo | Define expectativas de calidad en el cÃ³digo; recovery automÃ¡tico; linaje integrado |
| **Unity Catalog** | Gobierno de datos | Permisos por grupo Azure AD; linaje columna-a-columna; External Locations para ADLS |
| **Event Hubs** | Streaming de eventos | Eventos de pÃ³lizas vendidas y siniestros en near-real-time; Capture a ADLS como backup |
| **Fivetran** | Ingesta SaaS (Dynamics 365) | Sync incremental cada 15 min sin cÃ³digo; conector nativo certificado |
| **Power BI** | VisualizaciÃ³n y reportes | Connected via Partner Connect al SQL Warehouse; Import mode para ejecutivos, DirectQuery para operacional |
| **Azure Key Vault** | GestiÃ³n de secretos | Service Principal credentials, connection strings; vinculado a Databricks Secret Scopes |
| **Azure DevOps** | CI/CD | YAML pipelines: lint â†’ test â†’ deploy staging â†’ approval â†’ deploy prod |
| **Terraform** | Infrastructure as Code | azurerm provider para ADLS, ADF, Databricks workspace, Key Vault, VNet |
| **Azure Monitor** | Observabilidad | Alertas de fallos en pipelines, costos por encima de threshold, degradaciÃ³n de rendimiento |

---

## ðŸ”€ Flujo de Datos

### 1. Ingesta (Bronze)

```python
# ADF Copy Activity: CDC incremental desde SQL Server
# Configurado con Tumbling Window trigger cada hora
# Self-hosted IR para Oracle on-premise (no expuesto a internet)

# Event Hubs â†’ Databricks Structured Streaming
df_stream = (spark.readStream
    .format("eventhubs")
    .options(**eh_conf)
    .load()
    .select(
        col("body").cast("string").alias("payload"),
        col("enqueuedTime").alias("event_time")
    )
)

# Auto Loader para archivos que llegan a ADLS
df_files = (spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "json")
    .option("cloudFiles.inferColumnTypes", "true")
    .load("abfss://bronze@storage.dfs.core.windows.net/eventos/")
)
```

### 2. TransformaciÃ³n (Silver)

```python
import dlt
from pyspark.sql.functions import *

@dlt.table(comment="Clientes unificados cross-system (MDM)")
@dlt.expect_or_drop("doc_no_nulo", "documento_id IS NOT NULL")
@dlt.expect_or_drop("nombre_valido", "LENGTH(TRIM(nombre)) > 2")
@dlt.expect("email_formato", "email RLIKE '^.+@.+\\\\..+$'")
def silver_clientes():
    core = dlt.read("bronze_core_clientes")
    crm = dlt.read("bronze_crm_contactos")
    siniestros = dlt.read("bronze_siniestros_asegurados")
    
    union = (core
        .unionByName(crm, allowMissingColumns=True)
        .unionByName(siniestros, allowMissingColumns=True)
        .dropDuplicates(["documento_id"])
    )
    return union
```

### 3. AgregaciÃ³n (Gold)

```python
@dlt.table(comment="Siniestralidad por ramo y regiÃ³n - mensual")
def gold_siniestralidad():
    return (
        dlt.read("silver_siniestros")
        .join(dlt.read("silver_polizas"), "poliza_id")
        .groupBy("ramo", "region", date_trunc("month", "fecha_siniestro").alias("mes"))
        .agg(
            count("*").alias("num_siniestros"),
            sum("monto_reclamado").alias("total_reclamado"),
            sum("monto_pagado").alias("total_pagado"),
            avg("dias_resolucion").alias("avg_dias_resolucion")
        )
    )
```

### 4. OptimizaciÃ³n

```sql
-- Z-ordering en columnas mÃ¡s filtradas por Power BI
OPTIMIZE gold.siniestralidad ZORDER BY (region, ramo, mes);

-- Liquid Clustering para tablas con patrones de filtro cambiantes
ALTER TABLE gold.transacciones SET TBLPROPERTIES (
  'delta.enableDeletionVectors' = 'true'
);
ALTER TABLE gold.transacciones CLUSTER BY (pais, fecha);
```

---

## ðŸ“ Estructura del Repositorio

```
01-lakehouse-medallion/
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ main.tf                    # Terraform: ADLS, ADF, Databricks, Key Vault
â”‚   â”œâ”€â”€ variables.tf               # Variables parametrizadas
â”‚   â”œâ”€â”€ outputs.tf                 # Outputs (workspace URL, storage endpoints)
â”‚   â”œâ”€â”€ environments/
â”‚   â”‚   â”œâ”€â”€ dev.tfvars
â”‚   â”‚   â”œâ”€â”€ staging.tfvars
â”‚   â”‚   â””â”€â”€ prod.tfvars
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ adls/                  # MÃ³dulo ADLS Gen2 con containers + lifecycle
â”‚       â”œâ”€â”€ databricks/            # Workspace + cluster policies
â”‚       â””â”€â”€ adf/                   # Data Factory + linked services
â”‚
â”œâ”€â”€ adf/
â”‚   â”œâ”€â”€ pipeline/                  # Pipelines de ADF (JSON exportado)
â”‚   â”‚   â”œâ”€â”€ pl_ingesta_oracle.json
â”‚   â”‚   â”œâ”€â”€ pl_ingesta_sqlserver.json
â”‚   â”‚   â””â”€â”€ pl_master_orquestador.json
â”‚   â”œâ”€â”€ linkedService/             # Conexiones a fuentes y destinos
â”‚   â”œâ”€â”€ dataset/                   # Esquemas de entrada/salida
â”‚   â””â”€â”€ trigger/                   # Schedule + tumbling window triggers
â”‚
â”œâ”€â”€ databricks/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ ingesta/
â”‚   â”‚   â”‚   â”œâ”€â”€ stream_event_hubs.py
â”‚   â”‚   â”‚   â””â”€â”€ autoloader_archivos.py
â”‚   â”‚   â”œâ”€â”€ transformaciones/
â”‚   â”‚   â”‚   â”œâ”€â”€ dlt_bronze_to_silver.py
â”‚   â”‚   â”‚   â”œâ”€â”€ dlt_silver_to_gold.py
â”‚   â”‚   â”‚   â””â”€â”€ mdm_clientes.py
â”‚   â”‚   â””â”€â”€ calidad/
â”‚   â”‚       â”œâ”€â”€ expectations_silver.py
â”‚   â”‚       â””â”€â”€ reconciliacion_gold.sql
â”‚   â”œâ”€â”€ resources/
â”‚   â”‚   â”œâ”€â”€ workflows.yml          # Databricks Workflows definition
â”‚   â”‚   â””â”€â”€ dlt_pipeline.yml       # DLT pipeline config
â”‚   â””â”€â”€ databricks.yml             # Databricks Asset Bundle config
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_transformaciones.py
â”‚   â”‚   â””â”€â”€ test_calidad.py
â”‚   â””â”€â”€ integration/
â”‚       â””â”€â”€ test_pipeline_e2e.py
â”‚
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ alerts.bicep               # Azure Monitor alert rules
â”‚   â””â”€â”€ dashboard_costos.json      # Power BI dashboard de costos
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.png           # Diagrama de arquitectura (draw.io)
â”‚   â”œâ”€â”€ data-flow.md               # DocumentaciÃ³n del flujo de datos
â”‚   â””â”€â”€ runbook.md                 # Runbook operativo para on-call
â”‚
â”œâ”€â”€ azure-pipelines.yml            # CI/CD con Azure DevOps
â”œâ”€â”€ .pre-commit-config.yaml        # Pre-commit hooks (ruff, sqlfluff)
â””â”€â”€ README.md                      # Este archivo
```

---

## ðŸš€ Deployment

### Prerrequisitos

```bash
# Azure CLI + Terraform + Databricks CLI
az login
terraform init
databricks configure --token
```

### 1. Infraestructura (Terraform)

```bash
cd infra/
terraform plan -var-file=environments/dev.tfvars
terraform apply -var-file=environments/dev.tfvars
```

### 2. Databricks Assets (DABs)

```bash
cd databricks/
databricks bundle validate -t dev
databricks bundle deploy -t dev
```

### 3. ADF Pipelines

```bash
# ADF en Git mode: merge a main â†’ ARM template auto-generated
# Azure DevOps pipeline despliega ARM template a staging/prod
az deployment group create \
  --resource-group rg-data-prod \
  --template-file adf/ARMTemplateForFactory.json \
  --parameters adf/ARMTemplateParametersForFactory.json
```

### CI/CD Pipeline

```yaml
# azure-pipelines.yml (simplificado)
trigger:
  branches:
    include: [main]

stages:
- stage: CI
  jobs:
  - job: LintAndTest
    steps:
    - script: |
        pip install ruff pytest
        ruff check databricks/src/
        pytest tests/unit/ --junitxml=results.xml

- stage: DeployStaging
  dependsOn: CI
  jobs:
  - deployment: Staging
    environment: staging
    strategy:
      runOnce:
        deploy:
          steps:
          - script: databricks bundle deploy -t staging

- stage: DeployProd
  dependsOn: DeployStaging
  jobs:
  - deployment: Production
    environment: production  # Requires approval
    strategy:
      runOnce:
        deploy:
          steps:
          - script: databricks bundle deploy -t production
```

---

## ðŸ’° Costos Estimados

> Basado en precios pÃºblicos de Azure (East US 2) + Databricks Premium, febrero 2025.

| Servicio | ConfiguraciÃ³n | USD/mes |
|----------|---------------|---------|
| ADLS Gen2 | 3 TB Hot + lifecycle | $55 |
| Azure Data Factory | ~200 runs/dÃ­a + SSIS IR | $430 |
| Fivetran | 1 conector Standard | $200 |
| Event Hubs | Standard, 3 TU | $220 |
| Databricks Jobs Compute | i3.xlarge, Spot 70%, ~350 hrs | $700 |
| Databricks Streaming | i3.xlarge always-on, Spot | $380 |
| Databricks SQL Warehouse | Pro Medium, 1-4 scale | $1,200 |
| Power BI | 15 Pro + Premium Per User | $350 |
| Key Vault + Monitor | Secretos + alertas | $65 |
| Networking | Private endpoints, VNet | $30 |
| **TOTAL** | | **~$3,630/mes** |

### Optimizaciones aplicadas

- âœ… **Job Clusters** en producciÃ³n (se destruyen al terminar): -40% vs All-Purpose
- âœ… **Spot VMs** en 70% de workers: -80% en VMs de workers
- âœ… **Photon Engine**: reduce tiempo de ejecuciÃ³n 3-5x = menos DBU
- âœ… **ADLS lifecycle**: Bronze > 90d â†’ Cool, > 365d â†’ Archive
- âœ… **SQL Warehouse auto-scale**: 1 (idle) â†’ 4 (peak) â†’ 1

---

## ðŸ“Š MÃ©tricas de Performance

| MÃ©trica | Antes | DespuÃ©s | Mejora |
|---------|-------|---------|--------|
| Reportes regulatorios | 3 semanas manual | 25 minutos automÃ¡tico | **99.8% reducciÃ³n** |
| Latencia datos para BI | 7 dÃ­as | < 1 hora | **99.4% reducciÃ³n** |
| Duplicados de clientes | 18% no detectados | 0% (MDM automÃ¡tico) | **100% detecciÃ³n** |
| Pipeline uptime | ~85% (fallos SSIS) | 99.95% | **+15 puntos** |
| Costo de auditorÃ­a | 3 semanas de prep | 4 horas (Unity Catalog) | **99.2% reducciÃ³n** |
| ETL nocturno | 6 horas | 45 minutos | **87% reducciÃ³n** |

---

## ðŸ”® Mejoras Futuras

- [ ] **Liquid Clustering** en todas las tablas Gold para eliminar particionamiento fijo
- [ ] **Delta Sharing** para compartir datos con reaseguradores sin copiar
- [ ] **Databricks Serverless SQL** para eliminar idle costs en warehouse
- [ ] **Feature Store** para alimentar modelos actuariales directamente desde Gold
- [ ] **dbt-databricks** para las transformaciones SQL con tests y documentaciÃ³n
- [ ] **Azure Managed Grafana** para dashboards de monitoreo tÃ©cnico

---

## ðŸ“„ Licencia

Este proyecto es material de portafolio con fines demostrativos. Los datos son sintÃ©ticos y no contienen informaciÃ³n real de ninguna empresa.
